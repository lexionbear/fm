{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2900d09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/lexion/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...\n",
      "Creating extension directory /home/lexion/.cache/torch_extensions/py38_cu121/fm_scan...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/lexion/.cache/torch_extensions/py38_cu121/fm_scan/build.ninja...\n",
      "/home/lexion/anaconda3/envs/p2/lib/python3.8/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module fm_scan...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module fm_scan...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from fm.scan.fm_scan import fm_scan, fm_memory # c++ cuda kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930ff8da",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eea6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, dim, seqlen, mem = 1, 2048, 8192, 32\n",
    "\n",
    "# testing just the scan kernel\n",
    "gates = 0.9 + 0.1 * torch.rand(batch_size, mem, seqlen, device=\"cuda\", dtype=torch.bfloat16)\n",
    "tokens = torch.rand(batch_size, dim, seqlen, device=\"cuda\", dtype=torch.bfloat16)\n",
    "memory_states = fm_scan(gates, tokens, initial_state=None)\n",
    "print(memory_states.shape)\n",
    "\n",
    "# testing the memory layer\n",
    "alpha = torch.nn.functional.softmax(torch.rand(batch_size, seqlen, mem, device=\"cuda\", dtype=torch.bfloat16, requires_grad=True), dim=-1)\n",
    "update_scale = torch.rand(batch_size, seqlen, 1, device=\"cuda\", dtype=torch.bfloat16, requires_grad=True) + 0.001\n",
    "output_scale = torch.rand(batch_size, seqlen, 1, device=\"cuda\", dtype=torch.bfloat16, requires_grad=True) + 0.001\n",
    "inputs = torch.rand(batch_size, seqlen, dim, device=\"cuda\", dtype=torch.bfloat16, requires_grad=True)\n",
    "\n",
    "memory_output, memory_states = fm_memory(alpha, update_scale, output_scale, inputs, initial_state=None, mem_norm=True, norm_eps=1e-6)\n",
    "print(memory_states.shape)\n",
    "print(memory_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0e34ff",
   "metadata": {},
   "source": [
    "## Correctness test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4538a9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0012, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MeanBackward0>)\n",
      "tensor(0., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MeanBackward0>)\n",
      "tensor(0.9961, device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor(2.3125, device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "# check with pytorch\n",
    "from fm.scan.fm_pytorch import fm_scan_pytorch\n",
    "\n",
    "batch_size, dim, seqlen, mem = 1, 5, 1024, 1\n",
    "gates = 0.5 + 0.5 * torch.rand(batch_size, mem, seqlen, device=\"cuda\", dtype=torch.bfloat16)\n",
    "tokens = torch.rand(batch_size, dim, seqlen, device=\"cuda\", dtype=torch.bfloat16)\n",
    "\n",
    "tokens_cuda = tokens.clone().detach().requires_grad_()\n",
    "gates_cuda = gates.clone().detach().requires_grad_()\n",
    "tokens_pytorch = tokens.clone().detach().requires_grad_()\n",
    "gates_pytorch = gates.clone().detach().requires_grad_()\n",
    "\n",
    "# cuda val\n",
    "memory_states = fm_scan(gates_cuda, tokens_cuda, initial_state=None)\n",
    "loss_cuda = memory_states.sum()\n",
    "loss_cuda.backward()\n",
    "\n",
    "gates_cuda32 = gates_cuda.clone().to(torch.float32)\n",
    "tokens_cuda32 = tokens_cuda.clone().to(torch.float32)\n",
    "\n",
    "memory_states32 = fm_scan(gates_cuda, tokens_cuda, initial_state=None)\n",
    "loss_cuda32 = memory_states32.sum()\n",
    "loss_cuda32.backward()\n",
    "\n",
    "# pytorch val\n",
    "memory_states_pytorch = fm_scan_pytorch(gates_pytorch, tokens_pytorch, initial_state=None)\n",
    "loss_pytorch = memory_states_pytorch.sum()\n",
    "loss_pytorch.backward()\n",
    "\n",
    "print(torch.mean(torch.abs(memory_states - memory_states_pytorch)))\n",
    "print(torch.mean(torch.abs(memory_states32 - memory_states)))\n",
    "print(torch.mean(torch.abs(tokens_cuda.grad - tokens_pytorch.grad)))\n",
    "print(torch.mean(torch.abs(gates_cuda.grad - gates_pytorch.grad)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
