{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2900d09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from fm.scan.fm_scan import fm_scan, fm_memory # c++ cuda kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930ff8da",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eea6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, dim, seqlen, mem = 1, 2048, 8192, 32\n",
    "\n",
    "# testing just the scan kernel\n",
    "gates = 0.9 + 0.1 * torch.rand(batch_size, mem, seqlen, device=\"cuda\", dtype=torch.bfloat16)\n",
    "tokens = torch.rand(batch_size, dim, seqlen, device=\"cuda\", dtype=torch.bfloat16)\n",
    "memory_states = fm_scan(gates, tokens, initial_state=None)\n",
    "print(memory_states.shape)\n",
    "\n",
    "# testing the memory layer\n",
    "alpha = torch.nn.functional.softmax(torch.rand(batch_size, seqlen, mem, device=\"cuda\", dtype=torch.bfloat16, requires_grad=True), dim=-1)\n",
    "update_scale = torch.rand(batch_size, seqlen, 1, device=\"cuda\", dtype=torch.bfloat16, requires_grad=True) + 0.001\n",
    "output_scale = torch.rand(batch_size, seqlen, 1, device=\"cuda\", dtype=torch.bfloat16, requires_grad=True) + 0.001\n",
    "inputs = torch.rand(batch_size, seqlen, dim, device=\"cuda\", dtype=torch.bfloat16, requires_grad=True)\n",
    "\n",
    "memory_output, memory_states = fm_memory(alpha, update_scale, output_scale, inputs, initial_state=None, mem_norm=True, norm_eps=1e-6)\n",
    "print(memory_states.shape)\n",
    "print(memory_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0e34ff",
   "metadata": {},
   "source": [
    "## Correctness test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4538a9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check with pytorch\n",
    "from fm.scan.fm_pytorch import fm_scan_pytorch\n",
    "\n",
    "batch_size, dim, seqlen, mem = 1, 5, 32, 1\n",
    "gates = 0.5 + 0.5 * torch.rand(batch_size, mem, seqlen, device=\"cuda\", dtype=torch.float32)\n",
    "tokens = torch.rand(batch_size, dim, seqlen, device=\"cuda\", dtype=torch.float32)\n",
    "\n",
    "tokens_cuda = tokens.clone().detach().requires_grad_()\n",
    "gates_cuda = gates.clone().detach().requires_grad_()\n",
    "tokens_pytorch = tokens.clone().detach().requires_grad_()\n",
    "gates_pytorch = gates.clone().detach().requires_grad_()\n",
    "\n",
    "# cuda val\n",
    "memory_states = fm_scan(gates_cuda, tokens_cuda, initial_state=None)\n",
    "loss_cuda = memory_states.sum()\n",
    "loss_cuda.backward()\n",
    "\n",
    "# pytorch val\n",
    "memory_states_pytorch = fm_scan_pytorch(gates_pytorch, tokens_pytorch, initial_state=None)\n",
    "loss_pytorch = memory_states_pytorch.sum()\n",
    "loss_pytorch.backward()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41408236",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.sum(torch.abs(memory_states - memory_states_pytorch)))\n",
    "print(torch.sum(torch.abs(tokens_cuda.grad - tokens_pytorch.grad)))\n",
    "print(torch.sum(torch.abs(gates_cuda.grad - gates_pytorch.grad)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
